{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn8bu4JQiqXneVDAWDpoOv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deanjuandcunha/Naive-Bayes-model/blob/main/Naive%20Bayes%20model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Maf9GClj9iVM",
        "outputId": "9246a636-e924-4b7b-c27e-843cf80c8276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Tweets ::\n",
            "\u001b[92m\n",
            "@ZarlashtFaisal @Tabinda_Samar Sethi was HIGH ??? :)\n",
            "Fav if awake fam :)\n",
            "Just smile even your in Pain :) http://t.co/AxTiqf0xek\n",
            "camillus pleaseee? :)\n",
            "Why have i just painted my nails pink :) ???\n",
            "Negative Tweets ::\n",
            "\u001b[91m\n",
            "I need a big cuddle from Lew and kisses on my face :(((( I don't want to go through this again\n",
            "@kaiality too late now :(\n",
            "traffic :-(\n",
            "Soft defence by the best defensive team there :( #NRLTigersRoosters\n",
            "@LukeBryanOnline Yayyyy!!! I hope it's not while I am knocked out by anesthesia. I will be so sad if I miss it :(\n",
            "Raw tweet :\n",
            " @WforWoman 5. Over 20 W kurtas! And my Mom has about half the number I have :D #WSaleLove\n",
            "After processing the tweet : \n",
            " ['5', '20', 'w', 'kurta', 'mom', 'half', 'number', ':d', 'wsalelov']\n",
            "0.0\n",
            "9085\n",
            "\u001b[92m\n",
            "I am happy :: Positive sentiment (2.15)\n",
            "\u001b[91m\n",
            "I am bad :: Negative Sentiment (-1.29)\n",
            "\u001b[92m\n",
            "this movie should have been great. :: Positive sentiment (2.14)\n",
            "\u001b[92m\n",
            "great :: Positive sentiment (2.14)\n",
            "\u001b[92m\n",
            "great great :: Positive sentiment (4.28)\n",
            "\u001b[92m\n",
            "great great great :: Positive sentiment (6.41)\n",
            "\u001b[92m\n",
            "great great great great :: Positive sentiment (8.55)\n",
            "\u001b[91m\n",
            "I am not happy :( :: Negative Sentiment (-5.36)\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "np.random.seed(1)\n",
        "rand= np.random.randint(0, 5000, 5)\n",
        "print('Positive Tweets ::')\n",
        "print('\\033[92m')\n",
        "for i in rand:\n",
        "  print(all_positive_tweets[i])\n",
        "\n",
        "print('Negative Tweets ::')\n",
        "print('\\033[91m')\n",
        "for i in rand:\n",
        "  print(all_negative_tweets[i])\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def process_tweet(tweet):\n",
        "  stemmer = PorterStemmer() \n",
        "  stopwords_english = stopwords.words('english')\n",
        "\n",
        "  # remove the stock market tickers\n",
        "  tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "  # remove the old styles retweet text 'RT'\n",
        "  tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "  # remove the hyperlinks\n",
        "  tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
        "\n",
        "  # remove the # symbol\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "  # Tokenize the tweet\n",
        "  tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
        "  tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "  tweet_clean = []\n",
        "\n",
        "  # removing stopwords and punctuation\n",
        "  for word in tweet_tokens:\n",
        "    if (word not in stopwords_english and\n",
        "        word not in string.punctuation):\n",
        "      stem_word = stemmer.stem(word)    #stemming\n",
        "      tweet_clean.append(stem_word)\n",
        "\n",
        "  return tweet_clean\n",
        "\n",
        "tweet = all_positive_tweets [np.random.randint(0,5000)]\n",
        "print('Raw tweet :\\n',tweet)\n",
        "tweet = process_tweet(tweet)\n",
        "print('After processing the tweet : \\n', tweet)\n",
        "\n",
        "def count_tweets(tweets, ys):\n",
        "  ys_list = np.squeeze(ys).tolist()\n",
        "  freqs ={}\n",
        "\n",
        "  for y, tweet in zip(ys_list, tweets):\n",
        "    for word in process_tweet(tweet):\n",
        "      pair = (word, y)\n",
        "      if pair in freqs:\n",
        "        freqs[pair] +=1\n",
        "      else:\n",
        "        freqs[pair] = 1\n",
        "  \n",
        "  return freqs\n",
        "\n",
        "def lookup(freqs, word, label):\n",
        "  n = 0\n",
        "  pair = (word, label)\n",
        "  if pair in freqs:\n",
        "    n = freqs[pair]\n",
        "  return n \n",
        "\n",
        "# splitting the data for training and testing \n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# numpy array for the labels in the training set\n",
        "train_y = np.append(np.ones((len(train_pos))), np.zeros((len(train_neg))))\n",
        "test_y = np.append(np.ones((len(test_neg))), np.zeros((len(test_neg))))\n",
        "\n",
        "# Build a frequency dictionary\n",
        "freqs = count_tweets(train_x, train_y)\n",
        "\n",
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "  logliklihood = {}\n",
        "  logprior = 0\n",
        "\n",
        "  # calculate V, number of unique words in the vocabulary\n",
        "  vocab = set([pair[0] for pair in freqs.keys()])\n",
        "  V = len(vocab)\n",
        "\n",
        "  ## Calculate N_pos, N_neg, V_pos, V_neg\n",
        "  # N_pos : total number of positive words\n",
        "  # N_neg : total number of negative words\n",
        "  # V_pos : total number of unique positive words\n",
        "  # V_neg : total number of unique negative words\n",
        "\n",
        "  N_pos = N_neg = V_pos = V_neg = 0\n",
        "  for pair in freqs.keys():\n",
        "    if pair[1]>0:\n",
        "      V_pos +=1\n",
        "      N_pos += freqs[pair]\n",
        "    else:\n",
        "      V_neg +=1\n",
        "      N_neg += freqs[pair]\n",
        "\n",
        "  # Number of Documents (tweets)\n",
        "  D = len(train_y)\n",
        "\n",
        "  # D_pos, number of positive documnets\n",
        "  D_pos = len(list(filter(lambda x: x>0, train_y)))\n",
        "\n",
        "  # D_pos, number of negative documnets\n",
        "  D_neg = len(list(filter(lambda x: x<=0, train_y)))\n",
        "\n",
        "  # calculate the logprior\n",
        "  logprior = np.log(D_pos) - np.log(D_neg)\n",
        "\n",
        "  for word in vocab:\n",
        "    freqs_pos = lookup(freqs, word, 1)\n",
        "    freqs_neg = lookup(freqs, word, 0)\n",
        "\n",
        "    # calculte the probability of each word being positive and negative\n",
        "    p_w_pos = (freqs_pos+1)/(N_pos+V)\n",
        "    p_w_neg = (freqs_neg+1)/(N_neg+V)\n",
        "\n",
        "    logliklihood[word] = np.log(p_w_pos/p_w_neg)\n",
        "  \n",
        "  return logprior, logliklihood\n",
        "\n",
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))\n",
        "\n",
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "  word_l = process_tweet(tweet)\n",
        "  p = 0\n",
        "  p+=logprior\n",
        "\n",
        "  for word in word_l:\n",
        "    if word in loglikelihood:\n",
        "      p+=loglikelihood[word]\n",
        "\n",
        "  return p\n",
        "\n",
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "  accuracy = 0\n",
        "  y_hats = []\n",
        "  for tweet in test_x:\n",
        "    if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
        "      y_hat_i = 1\n",
        "    else:\n",
        "      y_hat_i = 0\n",
        "    y_hats.append(y_hat_i)\n",
        "  error = np.mean(np.absolute(test_y - y_hats))\n",
        "  accuracy = 1-error\n",
        "\n",
        "  return accuracy\n",
        "  \n",
        "  print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))\n",
        "  \n",
        "tweets = ['I am happy','I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great', 'I am not happy :(']\n",
        "for tweet in tweets:\n",
        "  p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
        "  if p>1:\n",
        "    print('\\033[92m')\n",
        "    print(f'{tweet} :: Positive sentiment ({p:.2f})')\n",
        "  else:\n",
        "    print('\\033[91m')\n",
        "    print(f'{tweet} :: Negative Sentiment ({p:.2f})')\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbn_hnp3Hxpo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}